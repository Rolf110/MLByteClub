{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RKxe88YT_p9P"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "E7_2lHue_r0x"
   },
   "outputs": [],
   "source": [
    "# fetch the dataset.\n",
    "digits, targets = load_digits(return_X_y=True)\n",
    "digits = digits.astype(np.float32) / 255\n",
    "\n",
    "digits_train, digits_test, targets_train, targets_test = train_test_split(digits, targets, random_state=0)\n",
    "\n",
    "train_size = digits_train.shape[0]\n",
    "test_size = digits_test.shape[0]\n",
    "\n",
    "input_size = 8*8\n",
    "classes_n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "feWQtoKtn1vV",
    "outputId": "c3bc4da0-711d-4648-a90c-560131283358"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 10\n",
    "\n",
    "sample_idx = np.random.choice(1797, N, replace=False)\n",
    "digits_sample = digits[sample_idx]\n",
    "targets_sample = targets[sample_idx]\n",
    "f, ax  = plt.subplots(1,10, figsize=(10, 5))\n",
    "\n",
    "for i in range(N):\n",
    "    ax[i].imshow(digits_sample[i].reshape(8,8))\n",
    "    ax[i].set_title('label: '+str(targets_sample[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ffpXAKqQ_vfg"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # Trainable parameters of the layer and their gradients\n",
    "        self.thetas = np.random.randn(input_size, output_size) # the weight matrix of the layer (W)\n",
    "        self.thetas_grads = np.empty_like(self.thetas) # gradient w.r.t. the weight matrix of the layer\n",
    "        self.bias = np.random.randn(output_size) # bias terms of the layer (b)\n",
    "        self.bias_grads = np.empty_like(self.bias) # gradient w.r.t. bias terms of the linear layer\n",
    "\n",
    "    def forward(self, x): \n",
    "        # keep x for backward computation\n",
    "        self.x = x\n",
    "        output = np.matmul(x, self.thetas) + self.bias\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def backward(self, output_grad, learning_rate):\n",
    "        \"\"\"\n",
    "        Calculate and return gradient of the loss w.r.t. the input of linear layer given the input x and the gradient \n",
    "        w.r.t output of linear layer. You should also calculate and update gradients of layer parameters.\n",
    "        :param x: np.array, input tensor for linear layer;\n",
    "        :param output_grad: np.array, grad tensor w.r.t output of linear layer;\n",
    "        :return: np.array, grad w.r.t input of linear layer\n",
    "        \"\"\"\n",
    "        \n",
    "        ### BEGIN Solution (do not delete this comment)\n",
    "        input_grad = np.dot(output_grad, self.thetas.T)\n",
    "        self.thetas_grads = np.outer(self.x, output_grad)\n",
    "        self.bias_grads = np.squeeze(output_grad)\n",
    "        # >>> your solution here <<<\n",
    "        self.step(learning_rate)\n",
    "\n",
    "        ### END Solution (do not delete this comment!)\n",
    "        \n",
    "        return input_grad\n",
    "\n",
    "    def step(self, learning_rate):\n",
    "        self.thetas -= self.thetas_grads * learning_rate\n",
    "        self.bias -= self.bias_grads * learning_rate\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.thetas, self.bias\n",
    "    \n",
    "    def set_weights(self, thetas, bias):\n",
    "        self.thetas = thetas\n",
    "        self.bias = bias\n",
    "\n",
    "\n",
    "class LogisticActivation:\n",
    "    def __init__(self):\n",
    "        # the layer has no parameters\n",
    "        pass \n",
    "        \n",
    "    def sig(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # keep o for backward computation\n",
    "        self.o = self.sig(x)\n",
    "        return self.o \n",
    "\n",
    "\n",
    "    def backward(self,  output_grad, learning_rate=None):\n",
    "        \"\"\"\n",
    "        Calculate and return the gradient of the loss w.r.t. the input\n",
    "        of logistic non-linearity (given input x and the gradient \n",
    "        w.r.t output of logistic non-linearity).\n",
    "        \n",
    "        :param x: np.array, input tensor for logistic non-linearity;\n",
    "        :param output_grad: np.array, grad tensor w.r.t output of logistic non-linearity;\n",
    "        :return: np.array, grad w.r.t input of logistic non-linearity\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        input_grad = output_grad * self.o *(1-self.o )\n",
    "\n",
    "        \n",
    "        return input_grad\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_layer_size, output_size):\n",
    "        self.linear1 = Linear(input_size, hidden_layer_size)\n",
    "        self.activation1 = LogisticActivation()\n",
    "        self.linear2 = Linear(hidden_layer_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.linear1.forward(x)\n",
    "        h1a = self.activation1.forward(h1)\n",
    "        self.watchdog_absmax = np.max(np.abs(h1a))\n",
    "        out = self.linear2.forward(h1a)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, output_grad, learning_rate):\n",
    "        \"\"\"\n",
    "        Calculate and return the gradient of the loss w.r.t. the input of MLP given the input and the gradient \n",
    "        w.r.t output of MLP. You should also update gradients of paramerters of MLP layers.\n",
    "        Hint - you should chain backward operations of modules you have already implemented. You may also\n",
    "        need to calculate intermediate forward results.\n",
    "        \n",
    "        :param x: np.array, input tensor for MLP;\n",
    "        :param output_grad: np.array, grad tensor w.r.t output of MLP;\n",
    "        :return: np.array, grad w.r.t input of MLP\n",
    "        \"\"\"\n",
    " \n",
    "        grad_l2 = self.linear2.backward(output_grad, learning_rate)\n",
    "        grad_sig = self.activation1.backward(grad_l2)\n",
    "        out = self.linear1.backward(grad_sig, learning_rate)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def get_weights(self):\n",
    "\n",
    "        return {\"1 layer\" : self.linear1.get_weights(),\n",
    "                \"2 layer\" : self.linear2.get_weights()}\n",
    "    \n",
    "    \n",
    "    def set_weights(self, w_h1, b_h1, w_h2, b_h2):\n",
    "        self.linear1.set_weights(w_h1, b_h1)\n",
    "        self.linear2.set_weights(w_h2, b_h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "07DUqp86_0To"
   },
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits, reference_answers):\n",
    "\n",
    "    true_answer = reference_answers\n",
    "    reference_answers = np.zeros(10)\n",
    "    reference_answers[true_answer] = 1\n",
    "\n",
    "    exps = np.exp(logits-np.max(logits))\n",
    "    softmax_probs = exps / np.sum(exps)\n",
    "    \n",
    "    loss = -np.sum(reference_answers*np.log(softmax_probs+1e-9))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def grad_softmax_crossentropy_with_logits(logits, reference_answers):\n",
    "\n",
    "    true_answer = reference_answers\n",
    "    reference_answers = np.zeros(10)\n",
    "    reference_answers[true_answer] = 1\n",
    "\n",
    "    exps = np.exp(logits-np.max(logits))\n",
    "    softmax_probs = exps / np.sum(exps)\n",
    "\n",
    "    grad = (softmax_probs - reference_answers)\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DWkD2V1y_4QU",
    "outputId": "025c7788-1647-41b7-b6c2-3d57dbf1663c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0 , Loss : 2.71 , Accuracy on train: 0.103 , Accuracy on test: 0.0667\n",
      "Starting epoch 10 , Loss : 2.15 , Accuracy on train: 0.625 , Accuracy on test: 0.649\n",
      "Starting epoch 20 , Loss : 1.55 , Accuracy on train: 0.821 , Accuracy on test: 0.807\n",
      "Starting epoch 30 , Loss : 1.07 , Accuracy on train: 0.875 , Accuracy on test: 0.871\n",
      "Starting epoch 40 , Loss : 0.742 , Accuracy on train: 0.9 , Accuracy on test: 0.889\n",
      "Starting epoch 50 , Loss : 0.534 , Accuracy on train: 0.911 , Accuracy on test: 0.904\n",
      "Starting epoch 60 , Loss : 0.399 , Accuracy on train: 0.92 , Accuracy on test: 0.924\n",
      "Starting epoch 70 , Loss : 0.308 , Accuracy on train: 0.927 , Accuracy on test: 0.931\n",
      "Starting epoch 80 , Loss : 0.245 , Accuracy on train: 0.929 , Accuracy on test: 0.931\n",
      "Starting epoch 90 , Loss : 0.198 , Accuracy on train: 0.936 , Accuracy on test: 0.933\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "mlp = MLP(input_size=input_size, hidden_layer_size=100, output_size=classes_n)\n",
    "\n",
    "epochs_n = 100\n",
    "learning_curve = [0] * epochs_n\n",
    "test_curve = [0] * epochs_n\n",
    "\n",
    "x_train = digits_train\n",
    "x_test = digits_test\n",
    "y_train = targets_train\n",
    "y_test = targets_test\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "for epoch in range(epochs_n):\n",
    "    \n",
    "    y_pred = []\n",
    "\n",
    "    for sample_i in range(train_size):\n",
    "        x = x_train[sample_i].reshape((1, -1))\n",
    "        target = np.array([y_train[sample_i]])\n",
    "\n",
    "        logits = mlp.forward(x)\n",
    "        \n",
    "        loss = softmax_crossentropy_with_logits(logits, target)\n",
    "\n",
    "        grads_input_sml = grad_softmax_crossentropy_with_logits(logits, target)\n",
    "\n",
    "        mlp.backward(grads_input_sml, learning_rate)\n",
    "\n",
    "        y_pred.extend(logits.argmax(1))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        \n",
    "        y_pred_test = []\n",
    "\n",
    "        for sample_i in range(test_size):\n",
    "            x = x_test[sample_i].reshape((1, -1))\n",
    "            target = np.array([y_test[sample_i]])\n",
    "            \n",
    "            logits = mlp.forward(x)\n",
    "            y_pred_test.extend(logits.argmax(1))\n",
    "\n",
    "        print('Starting epoch {}'.format(epoch), \\\n",
    "              ', Loss : {:.3}'.format(loss), \\\n",
    "              ', Accuracy on train: {:.3}'.format(accuracy_score(y_train, y_pred)), \\\n",
    "              ', Accuracy on test: {:.3}'.format(accuracy_score(y_test, y_pred_test)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_accuracy(nn, y_test):\n",
    "    \n",
    "    y_test = np.array(y_test)\n",
    "    y_pred_test = np.array([])\n",
    "\n",
    "    for sample_i in range(y_test.size):\n",
    "        x = x_test[sample_i].reshape((1, -1))\n",
    "\n",
    "        logits = nn.forward(x)\n",
    "        y_pred_test = np.append(y_pred_test, logits.argmax(1))\n",
    "\n",
    "    return accuracy_score(y_test, y_pred_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0DNQhxXaARCy",
    "outputId": "696177f9-461a-40d4-bfd8-a984c4b528cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 39  0  0  0  0  2  0  0  2]\n",
      " [ 0  1 41  1  0  0  0  1  0  0]\n",
      " [ 0  0  0 43  0  0  0  0  1  1]\n",
      " [ 0  0  0  0 37  0  0  1  0  0]\n",
      " [ 0  0  0  0  0 46  0  0  0  2]\n",
      " [ 0  1  0  0  0  0 51  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 48  0  0]\n",
      " [ 0  4  1  0  0  1  0  1 41  0]\n",
      " [ 0  0  0  4  0  2  0  0  1 40]]\n"
     ]
    }
   ],
   "source": [
    "logits = mlp.forward(x_test)\n",
    "exps = np.exp(logits-np.max(logits))\n",
    "softmax_probs = exps / np.sum(exps)\n",
    "\n",
    "pred = np.zeros((y_test.shape[0]), dtype=\"int\")\n",
    "\n",
    "for idx, prob in enumerate(softmax_probs):\n",
    "    pred[idx] = np.argmax(prob)\n",
    "\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n",
      "Accuracy on test, initial mlp: 0.94\n"
     ]
    }
   ],
   "source": [
    "w_1 = mlp.get_weights()['1 layer'][0]\n",
    "b_1 = mlp.get_weights()['1 layer'][1]\n",
    "w_2 = mlp.get_weights()['2 layer'][0]\n",
    "b_2 = mlp.get_weights()['2 layer'][1]\n",
    "\n",
    "for m in [w_1, b_1, w_2, b_2]:\n",
    "    print(m.shape)\n",
    "print('Accuracy on test, initial mlp: {:.4}'.format(MLP_accuracy(mlp, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_factorized:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # Trainable parameters of the layer and their gradients\n",
    "        self.thetas = np.random.randn(input_size, output_size) # the weight matrix of the layer (W)\n",
    "        self.thetas_grads = np.empty_like(self.thetas) # gradient w.r.t. the weight matrix of the layer\n",
    "        self.bias = np.random.randn(output_size) # bias terms of the layer (b)\n",
    "        self.bias_grads = np.empty_like(self.bias) # gradient w.r.t. bias terms of the linear layer\n",
    "\n",
    "    def forward(self, x): \n",
    "        # keep x for backward computation\n",
    "        \n",
    "        self.scale_input = (np.max(np.abs(x))/(2**(8-1)-1))       \n",
    "        self.x_qzed = np.clip(np.round(x/self.scale_input), -2**(8-1) , 2**(8-1)-1)\n",
    "        output = (self.x_qzed.astype(\"int16\") @ self.A.astype(\"int16\") @ self.B.astype(\"int16\").T) * (self.scale_w ** 2) * self.scale_input + self.bias * self.scale_b\n",
    "        return output\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.scale_input, self.A, self.B, self.scale_w, self.bias, self.scale_b\n",
    "    \n",
    "    def set_weights(self, inp, w_pack, b_pack):\n",
    "        self.scale_input = inp\n",
    "        self.A = w_pack[0].astype(\"int8\")\n",
    "        self.B = w_pack[1].astype(\"int8\")\n",
    "        self.scale_w = w_pack[2]\n",
    "        self.bias = b_pack[0].astype(\"int8\")\n",
    "        self.scale_b = b_pack[1]\n",
    "\n",
    "class MLP_factorized:\n",
    "    def __init__(self, input_size, hidden_layer_size, output_size):\n",
    "        self.linear1 = Linear_factorized(input_size, hidden_layer_size)\n",
    "        self.activation1 = LogisticActivation()\n",
    "        self.linear2 = Linear_factorized(hidden_layer_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.linear1.forward(x)\n",
    "        h1a = self.activation1.forward(h1)\n",
    "        out = self.linear2.forward(h1a)\n",
    "        return out\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return {\"1 layer\" : self.linear1.get_weights(),\n",
    "                \"2 layer\" : self.linear2.get_weights()}\n",
    "        \n",
    "    def set_weights(self, in1, w1_pack, b1_pack, in2, w2_pack, b2_pack):\n",
    "        self.linear1.set_weights(in1, w1_pack, b1_pack)\n",
    "        self.linear2.set_weights(in2, w2_pack, b2_pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tensorly.decomposition import parafac\n",
    "\n",
    "def quantization_forw(T,z,b,scale):\n",
    "    T = np.clip(np.round(T/scale)+z,-2**(b-1),2**(b-1)-1)\n",
    "    return T.int()\n",
    "\n",
    "def quantization_back(T,z,scale):\n",
    "    return scale * (T - z)\n",
    "\n",
    "def ADMM(B, U, K, G, rank, scale):\n",
    "    ro = torch.trace(G) / rank\n",
    "\n",
    "    LL = G + ro * torch.eye(*G.size())\n",
    "    L = torch.linalg.cholesky(LL)\n",
    "    LL_inv = torch.cholesky_inverse(L)\n",
    "\n",
    "    r = torch.inf\n",
    "    s = torch.inf\n",
    "\n",
    "    for _ in range(100):\n",
    "    # while (r > 0.01) or (s > 0.01):\n",
    "        # print(r, s)\n",
    "        B_ = LL_inv @ (K + ro * (B + U)).T\n",
    "        B0 = torch.clone(B)\n",
    "\n",
    "        B = quantization_back(quantization_forw(B_.T - U, 0, 8, scale), 0, scale)\n",
    "        U = U + B - B_.T\n",
    "\n",
    "        r = torch.norm(B - B_.T, p='fro')**2 / torch.norm(B, p='fro')**2\n",
    "        s = torch.norm(B - B0, p='fro')**2 / torch.norm(U, p='fro')**2\n",
    "    return B, U\n",
    "\n",
    "\n",
    "def e_quant(X, A, B):\n",
    "    return torch.norm(X - A@B.T, p='fro')/torch.norm(X, p='fro')\n",
    "\n",
    "def factorize(W, reduction_rate = 2):\n",
    "    M,N = W.shape\n",
    "    rank = round(M*N/(M + N)/reduction_rate)\n",
    "    #W - должен быть просто numpy массивом\n",
    "    # weights, factors = parafac(W, rank=rank)\n",
    "    weights, factors = parafac(W, rank=np.min([M,N]))\n",
    "    A = torch.from_numpy(factors[0])\n",
    "    B = torch.from_numpy(factors[1])\n",
    "    new_W = A@B.T\n",
    "\n",
    "    W = torch.from_numpy(W)\n",
    "\n",
    "    U_A = torch.zeros_like(A)\n",
    "    U_B = torch.zeros_like(B)\n",
    "\n",
    "    e_old = 0.\n",
    "    e_new = e_quant(W,A,B)\n",
    "\n",
    "    b=8\n",
    "    #scale1 = (torch.max(A)-torch.min(A))/(2**b-1)\n",
    "    #scale2 = (torch.max(B)-torch.min(B))/(2**b-1)\n",
    "    #q_max = 9.89675982 * torch.mean(torch.abs(A - A.mean()))\n",
    "    #scale2 = 2*q_max/(2**b-1)\n",
    "    #q_max = 9.89675982 * torch.mean(torch.abs(B - B.mean()))\n",
    "    #scale1 = 2*q_max/(2**b-1)\n",
    "    q_max = 9.89675982 * torch.mean(torch.abs(W - W.mean()))\n",
    "    # scale = 2*q_max/(2**b-1)\n",
    "\n",
    "    scale = (torch.max(torch.abs(W))/(2**(8-1)-1))\n",
    "\n",
    "    while torch.abs(e_new - e_old) > 0.01:\n",
    "        # print(f'error: {torch.abs(e_new - e_old)}')\n",
    "        K = W @ B\n",
    "        G = B.T @ B\n",
    "        A, U_A = ADMM(A, U_A, K, G, rank, scale)  \n",
    "        \n",
    "        K = W.T @ A\n",
    "        G = A.T @ A\n",
    "        B, U_B = ADMM(B, U_B, K, G, rank, scale)\n",
    "\n",
    "        e_old = e_new\n",
    "        e_new = e_quant(W,A,B)\n",
    "    \n",
    "    # print(f'error: {torch.abs(e_new - e_old)}')\n",
    "\n",
    "    return new_W, A, B, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now hardcode for int8\n",
    "\n",
    "mlp_modified = MLP_factorized(input_size=input_size, hidden_layer_size=100, output_size=classes_n)\n",
    "\n",
    "w_1 = torch.from_numpy(mlp.get_weights()['1 layer'][0])\n",
    "new_w1, A_w1, B_w1, scale_w1 = factorize(w_1.numpy())\n",
    "A_w1_int = (A_w1/scale_w1).int()\n",
    "B_w1_int = (B_w1/scale_w1).int()\n",
    "w1_pack = [A_w1_int.numpy(), B_w1_int.numpy(), scale_w1.numpy()]\n",
    "\n",
    "b_1 = torch.from_numpy(mlp.get_weights()['1 layer'][1])\n",
    "scale_b1 = (torch.max(torch.abs(b_1))/(2**(8-1)-1))\n",
    "b1_int = quantization_forw(b_1, 0, 8, scale_b1)\n",
    "b1_pack = [b1_int.numpy(), scale_b1.numpy()]\n",
    "\n",
    "w_2 = torch.from_numpy(mlp.get_weights()['2 layer'][0])\n",
    "new_w2, A_w2, B_w2, scale_w2 = factorize(w_2.numpy())\n",
    "A_w2_int = (A_w2/scale_w2).int()\n",
    "B_w2_int = (B_w2/scale_w2).int()\n",
    "w2_pack = [A_w2_int.numpy(), B_w2_int.numpy(), scale_w2.numpy()]\n",
    "\n",
    "b_2 = torch.from_numpy(mlp.get_weights()['2 layer'][1])\n",
    "scale_b2 = (torch.max(torch.abs(b_2))/(2**(8-1)-1))\n",
    "b2_int = quantization_forw(b_2, 0, 8, scale_b2)\n",
    "b2_pack = [b2_int.numpy(), scale_b2.numpy()]\n",
    "\n",
    "\n",
    "input_scale = (np.max(np.abs(x_train))/(2**(8-1)-1))\n",
    "l2_input_scale = mlp.watchdog_absmax/(2**(8-1)-1)\n",
    "\n",
    "mlp_modified.set_weights(input_scale, w1_pack, b1_pack, l2_input_scale, w2_pack, b2_pack)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.51151752, -0.24669025,  0.61271747,  0.38412863, -0.67242104,\n",
       "       -1.87939557, -0.15523765, -1.08066435, -0.89635465, -0.63823596])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_modified.forward(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.96355181,  1.00562364,  6.82339377,  2.54897489, -4.32331601,\n",
       "        0.21446757, -1.36363943, -1.29536591,  1.21605275, -2.12493598])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.forward(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test, quantized mlp: 0.08222\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy on test, quantized mlp: {:.4}'.format(MLP_accuracy(mlp_modified, y_test)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PART_2_of_HW2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
