{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RKxe88YT_p9P"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "E7_2lHue_r0x"
   },
   "outputs": [],
   "source": [
    "# fetch the dataset.\n",
    "digits, targets = load_digits(return_X_y=True)\n",
    "digits = digits.astype(np.float32) / 255\n",
    "\n",
    "digits_train, digits_test, targets_train, targets_test = train_test_split(digits, targets, random_state=0)\n",
    "\n",
    "train_size = digits_train.shape[0]\n",
    "test_size = digits_test.shape[0]\n",
    "\n",
    "input_size = 8*8\n",
    "classes_n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "feWQtoKtn1vV",
    "outputId": "c3bc4da0-711d-4648-a90c-560131283358"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy0AAACDCAYAAACA7vQFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAilklEQVR4nO3de1iUZf4/8PdwGlDAI4IojKdIl0RbRdM0bcWfWZudLFMzqy2ttL3MNtPVFS3LDt+K72ZlWWGlmXZ0r++2mnlK0azsZKaiRIqSKFpyUkDm/v3BMjGC9z1zz+F5Hnq/rmuucj7PPPPhzXOYm5l5bpsQQoCIiIiIiMikQoxugIiIiIiISIaDFiIiIiIiMjUOWoiIiIiIyNQ4aCEiIiIiIlPjoIWIiIiIiEyNgxYiIiIiIjI1DlqIiIiIiMjUOGghIiIiIiJT46CFiIiIiIhMzdBBy9KlS2Gz2fDTTz95/dihQ4fioosu8ms/nTp1wm233ebXdQYKs9PH7PQxO33MTh+z08Pc9DE7fcxOH7OT4zstfvToo49i1KhRiI+Ph81mw7x584xuyTKcTieefPJJdO7cGZGRkUhLS8OKFSuMbssy8vLyMG7cOLRr1w5RUVG44IILMHv2bKPbMj1ud/p4vNNTWFiIW265BRdeeCFiYmLQsmVL9OvXD6+//jqEEEa3Z1rMTd+8efNgs9nOe8vJyTG6RdPjOdZ7gdhnw/zc4+/anDlzkJCQgIsvvhhr1641uh1LmT17Nh5//HHcddddSE9Px+rVqzFu3DjYbDbcfPPNRrdnat988w2GDh2KDh064IEHHkCbNm1w6NAhFBQUGN2a6XG708fjnZ7i4mIcPnwYo0ePRnJyMqqrq7Fu3Trcdttt2LdvHx577DGjWzQl5qbv+uuvR7du3Rrc//e//x1lZWVIT083oCvr4DlWTyD2WQ5a/Cg/Px+dOnVCcXEx4uLijG7HMo4cOYKnn34aU6ZMwaJFiwAAd955J4YMGYIHH3wQN954I0JDQw3u0pycTicmTJiA7t27Y+PGjYiKijK6JcvgducbHu/0pKWlYdOmTW73TZ06FVdffTX++c9/4pFHHuF21wjmpi8tLQ1paWlu9xUUFODw4cO48847ERERYVBn5sdzrL5A7LOm+3jY6tWrcdVVVyExMRF2ux1du3bFI488gpqamkaX37lzJwYOHIioqCh07twZixcvbrBMZWUlMjMz0a1bN9jtdiQlJWHGjBmorKxU9pOXl4e8vDyPeu/UqZNHywWKVbNbvXo1qqurce+997rus9lsuOeee3D48GFs375duQ5fWTW7jz/+GN9//z0yMzMRFRWFioqK8/YcKFbNjttdQzzeBSe7xnTq1AkVFRWoqqrSXocnmJu+ppTdihUrIITA+PHjtR7vLatmx3NsQ0bus6Z7p2Xp0qWIjo7G9OnTER0djQ0bNmDu3LkoKSnBU0895bbsL7/8giuvvBI33XQTxo4di1WrVuGee+5BREQE7rjjDgC1o+RRo0Zh69atmDRpEnr06IFdu3bh2WefRW5uLj788ENpP8OGDQMArS9FBZtVs/v666/RvHlz9OjRw+3+fv36ueqDBg3yIgnvWTW7Tz75BABgt9vRt29f7Ny5ExEREbjuuuvwwgsvoHXr1nqBeMGq2XG7a4jHu+Bld/r0aZSXl6OsrAybN29GdnY2BgwYEPC/5DI3fVbPrr7ly5cjKSkJl112mdeP1WHV7HiObcjQfVYYKDs7WwAQ+fn5rvsqKioaLDd58mTRrFkzcebMGdd9Q4YMEQDE008/7bqvsrJS9O7dW7Rr105UVVUJIYR48803RUhIiNiyZYvbOhcvXiwAiJycHNd9DodDTJw40W05h8MhHA6HVz/X8ePHBQCRmZnp1eO80ZSyu+qqq0SXLl0a3F9eXi4AiJkzZyrX4Y2mlN2oUaMEANGmTRsxfvx48e6774p//OMfIiwsTAwcOFA4nU7lOrzRlLLjdsfjnZHZLVy4UABw3YYNGyYOHTrk8eM9wdz0NcXs6nz//fcCgJgxY4bXj/VEU8qO51hz7bOm+3hY/ZFXaWkpiouLMXjwYFRUVGDv3r1uy4aFhWHy5Mmuf0dERGDy5Mk4duwYdu7cCQB455130KNHD3Tv3h3FxcWu25/+9CcAwMaNG6X9/PTTT5b4qyNg3exOnz4Nu93e4P7IyEhXPdCsml1ZWRkAID09HcuWLcMNN9yAhx9+GI888gi2bduG9evXe/Tz+8Kq2XG7a4jHu+BlN3bsWKxbtw5vvfUWxo0bB4DbnCeMyg2wfnZ1li9fDgBB+2gYYN3seI5tyMh91nQfD9u9ezfmzJmDDRs2oKSkxK126tQpt38nJiaiefPmbvelpKQAqA31kksuwf79+7Fnz57zflH02LFjfuzeWFbNLioqqtHPUZ45c8ZVDzQrZwfUHhTqGzduHGbNmoVt27YhIyPDL891PlbOjtuddVk9O4fDAYfDAaB2/500aRIyMjKwb9++gG57zE2f1bMDACEE3nrrLVx00UUNvpwfSFbNjudY3/lznzXVoOXXX3/FkCFDEBsbi4cffhhdu3ZFZGQkvvrqKzz00ENwOp1er9PpdKJnz5545plnGq0nJSX52rYpWDm79u3bY+PGjRBCwGazue7/+eefAdTuhIFk5ezqsomPj3e7v127dgBqP98aSFbOjtuddTXF7EaPHo0lS5bg008/xYgRIwLyHMxNX1PJLicnBwcPHsTChQv9vu7zsXJ2PMf6ny/7rKkGLZs2bcKJEyfw/vvvu305LD8/v9HlCwsLUV5e7jaqzM3NBfDblW26du2Kb7/9FsOGDXN7YdLUWDm73r1745VXXsGePXvwhz/8wXX/jh07XPVAsnJ2ffr0wZIlS3DkyJEGPQII+KVorZwdtzvraorZ1X1c4ty/nPoTc9PXVLJbvnw5bDab62M6wWDl7HiO9T9f9llTfael7nrNot5MmVVVVXjhhRcaXf7s2bN46aWX3JZ96aWXEBcXhz59+gAAbrrpJhw5cgRLlixp8Pi6KxrI+Hppt2CxcnbXXHMNwsPD3XoVQmDx4sXo0KEDBg4cqFyHL6yend1uR3Z2tttfXF555RUAwPDhw5Xr8IXVs+N2547Hu8Bnd/z48Ubvf/XVV2Gz2fDHP/5RuQ5dzE2flbOrU11djXfeeQeDBg1CcnKyx4/zlZWz4zm2ISP3WVO90zJw4EC0atUKEydOxF//+lfYbDa8+eabbr+s+hITE/HEE0/gp59+QkpKClauXIlvvvkGL7/8MsLDwwEAEyZMwKpVq3D33Xdj48aNuPTSS1FTU4O9e/di1apVWLt2Lfr27Xvenry5tNubb76JgwcPoqKiAgDw6aefYsGCBa4+6j7TFwhWzq5jx46YNm0annrqKVRXVyM9PR0ffvghtmzZguXLlwd8wjArZ5eQkIDZs2dj7ty5uOKKK3Dttdfi22+/xZIlSzB27NiAz3Rs5ey43TXE413gs3v00UeRk5ODK664AsnJyTh58iTee+89fPHFF7jvvvsanbncX5ibPitnV2ft2rU4ceJEUL+AD1g7O55jGzJ0n9W65pifNHZpt5ycHHHJJZeIqKgokZiYKGbMmCHWrl0rAIiNGze6lhsyZIhITU0VX375pRgwYICIjIwUDodDLFq0qMHzVFVViSeeeEKkpqYKu90uWrVqJfr06SPmz58vTp065VrO10u71V1urrFb/d79oallV1NTIx577DHhcDhERESESE1NFcuWLfMmEo81teycTqd47rnnREpKiggPDxdJSUlizpw5rssb+lNTy47b3US3x/J4F/jsPv74Y/HnP/9ZJCYmivDwcBETEyMuvfRSkZ2dHZTLpzI3zzSl7OrcfPPNIjw8XJw4ccLjx+hoatnxHDvR7bFG7rM2Ic4zXCMiIiIiIjIBU32nhYiIiIiI6FwctBARERERkalx0EJERERERKbGQQsREREREZkaBy1ERERERGRqHLQQEREREZGpBXVySafTicLCQsTExMBmswXzqU1FCIHS0lIkJiYiJMSzcSOzq8Xs9DE7fcxOj05uALMDuM35gtnpY3b6mJ0er3LTmdxl0aJFwuFwCLvdLvr16yd27Njh0eMKCgrOOxnZ7/HWu3dvZsfsgn7717/+5VFuzI7Z+evmzf7K7PSzY27Mzl83HuuYXbBvBQUFyry8fqdl5cqVmD59OhYvXoz+/fsjKysLI0aMwL59+9CuXTvpY2NiYgAAg3AlwhDu7VP7TdXqJGl9UtIWaf21v45SPkfI1u8a3HcMR7AHX6EbUrEfu9CrV6+gZvfLLf2Uy7wy65/S+lNHh0vrJ8ZGSus1x44re2iM0dlVXN1Xuczap7Ol9YFZd0rr8S/s8KonT9TlloI0NEcMvsIWTJgwAbm5ucrcAP9kd2RZD+UyF7c/LK1//XNHab3DLXu86skTZsjOH1T5j+76tbS+avUQ5XMkPe6+7fqyvwLBy67o3v7S+htT/lda9/V4CDQ8Jhp9rPvx8XTlMpek79Vad50rW+2S1q16jvWEc1CatP7juFBpfVfGGz73MPqa693+XVSyF7sL/4PuCRloHtEWXx5cbspjnWp/vXbCZmk950H5a6DGtikVM5wnQtvFKZeZsOYLaf2a6FJpvdf7d0jrXWbK1y9zFtXYio9cWch4PWh55plncNddd+H2228HACxevBj//ve/8dprr2HmzJnSx9a99RWGcITZjDuJO5vbpfVmMfKDRliY+kQU0sjPd1jkoQM6owO6YD92ISsrC+vWrQtadqER6r6jY+RvzYWXRkjrYSHyuk2zd6OzCwtXZxeryC7ULl9HIPaJutySbN1wVlQDAJo1a+ZRboCftrtm8v0NAMKby7cb1Tqaanb+oMouMlreW2ikets/9+fzZX8Fgpedap8M9PEQaHhMNPpYF+LB71u1v6o01XOsJ5yKny0kSp6N6jzjibBQ92NCwcmv0LFVbyS36YuzNZUAzHmsU+2vqmOZartqbJtSMcN5ItSD44xqn4uNlm9XquOCT793UfsfTz4e59WgpaqqCjt37sSsWbNc94WEhCAjIwPbt29vsHxlZSUqKytd/y4pKfHm6ZoUp3CiFL+iE7q77mN2nmF2ehrLDQCGDh3aaG4As6vD7PR5u78CzK4Oj3X6mJ0ep7MGJad/Rue4gW7381inxvNE8Hk1ZC8uLkZNTQ3i4+Pd7o+Pj8fRo0cbLL9w4UK0aNHCdUtKkn8sqymrRiUEBCLgPlpldmrMTs/5couLi2s0N4DZ1WF2+rzdXwFmV4fHOn3MTk9VTQUEBOxhzd3u57FOjeeJ4AvoJY9nzZqFU6dOuW4FBQWBfLomhdnpY3b6mJ0+ZqeP2elhbvqYnT5mp4/Z+carj4e1bdsWoaGhKCoqcru/qKgICQkJDZa32+2w29WfZ/89CIcdNthQhTMAYl33Mzs1ZqfHPbffHD9+vNHcAGZXh9np83Z/BZhdHR7r9DE7PRGhzWCDDZVny93u57FOjeeJ4PNq0BIREYE+ffpg/fr1uPbaawHUXmN6/fr1mDp1aiD60xKaeqG0vjF1pU/rf7W4QrlMzTn/DrGFIEa0xEkcQ2vUXlEi2NmdTBPKZUa+/4C0Hn1Q/uZcrw+/l9aLBihbaMAM2RWnyb/E5omIU+r8/al+bu3QwXX/5s2bcd999/nteZxDLpbWdw+QX1XNIw55efB1k6X1Zh94d2W2YGXnDydvl+9Uuwe86NP61w+SH0/PFaz9teI6+ZWEOs9QX1FureMFaf3WgyOl9Tccn0rrw7vfruwhpOjYb/9vgmNd8kU/K5cpKGslrR8riZbWVblZ9Ryr2iYBYMvzL0nrC4q7S+uX775GWt+YulrZA4p/cf1vCIDY8DicLN6H+Kp2gLMKQPCPdaHx6ittzb13mbS+6ODl0nrEZvmVEr1lmvNEW/n+CABzd10trT/+f7HS+pOzlkvrL9/fRdmDP3h99bDp06dj4sSJ6Nu3L/r164esrCyUl5e7riZG55eMFPyALxD9378C3X///czOQ8xOT11usaIVmv83O+bmGWanj/urPmanj9npcTTrhe9PbUBseByiQ2tfADM3z/A8EVxeD1rGjBmD48ePY+7cuTh69Ch69+6NNWvWNPhyPjWUYEtCtahEPmqvcb9r1y5m5yFmp6cutx/xAyr/+xb2+++/z9w8wOz0cX/Vx+z0MTs97aMuQJXzDA6Ufo5KZ+27XDzWeYbnieDyetACAFOnTjXVx8GsJMnWDe2FA5uwGhs2bEBsrPwtOfoNs9OTZOuGJNReQ34TVqNvX/VEmVSL2enj/qqP2eljdnoczXvC0bwnzjqrsP7YKzzWeYHnieAJ6NXDiIiIiIiIfMVBCxERERERmRoHLUREREREZGoctBARERERkalpfRHf7E5nnVEvJPFemfyLezW79/m0/kBRzU/T+jubch2ts7f71sQYeXaqHgFz5pt+hXz+GQC49eBl0nrcR3nS+rnzDljF4aFR0npudbm0DgBTbpVf2OPHa+WTcd0+f5O0vuWDSGUPZuTJvA8zFdfP99XRrR2UyyTjYEB7aMyrWc9I6ynhzZXr6PXkvdJ67EHFXvm8fL4RK4oY7vvvsuI1+ReRVXORmPEc4InIk1XKZdJn3yOtq87BJ6fJ98cF8fJsAaCm3txADWqiWvn4QMi/u5tymZ72D6T1qGny47xVz7Eqnuwvjrny1175o+WvD3vaVfM3BWeeFr7TQkREREREpsZBCxERERERmRoHLUREREREZGoctBARERERkalx0EJERERERKbGQQsREREREZkaBy1ERERERGRqHLQQEREREZGpWW5ySeeQi5XLbEzN9uk5zDKJjrdKU1pK655MQvdytm8/W85nf5DWo4erx8kJu31qISDecKgnkeu68m5pvVvRZ/5qx1SqWjql9QWFI5XrCNn8tbTebbP88T1yC6X1bfGXKHuQTbgWKKrj2bQnVijXcUN0ib/aaVTb78w5Jdtfpk2X1s+0VB9rOnx0QFrP+lw+md2tB+Xbtmq7tqpJuT9K6zdEfyOtqyaXtCpPft9xxYpJ/jIHSusLxi+T1v+29SZlDyn4UrlMsIWknVIuozqXWHVS0mA4MryNtL5n8gvSeur2SdJ6RwTnhRvfaSEiIiIiIlPjoIWIiIiIiEyNgxYiIiIiIjI1DlqIiIiIiMjUOGghIiIiIiJT46CFiIiIiIhMjYMWIiIiIiIyNdPN0xIa305aH/zc9oD3sOpUn4A/RyAUp4VK64sOXq5cRwQO+tSDs8VZab2qRYRP6w+Uiuv6K5b4RrmO5Ivk8/uEpsqvz89rzOubs/wWab1qpnwuGQDodn/w52k5PDRKWg/0HCxW1uyDHfK6B+tQzYmREt5cWt9ZmCStO+KPK3swYn4gX8349zh5XfH4vDGLpfXB101W9qD6/RtBfR4Btjz/kk/PkVtdLq13e92c8yqpjL1gp3KZm1rIl0kplO+vqvmBVj+rfo3UOjvwr0EDISFrm7Q+/OvbpfWHl7wnrWddN1bZgz/2Wb7TQkREREREpsZBCxERERERmRoHLUREREREZGoctBARERERkalx0EJERERERKbGQQsREREREZkaBy1ERERERGRqppun5cjYbtL6nLYfK9fRec2d0vraYf/rVU9W0XHTaWl9yA3qeUC2INKnHgb3yJXWf369q0/rD5QzLX0fv29MXS1fYJ28rLqG/LbhycoejJjzofV3Nmk9ZZi6pyIft7u238nnJii8TN6jUZLny6+df+W7Y5TrGLjiW2l9Ttu9XvV0rsiTVT493ijOIRcrl+n87glpffB38vlCPsh6Rlq/5u4HlT0kzzfXPC2e5Nbt/s98exL1Zm1Jh69Wz5HSdeXd0rpqvq+k6F+k9ZDNXyt7MKMV+9Xz480ZID+Wpc++R1qPHlcorc+ctVzZw8vZXZTLBNvg784ol9mSJj/HqrabrIfk87B0nrFH2UPRB8pFlLwatMybNw/z5893u+/CCy/E3r2+nRR/D/LEbuTD/Zfat29f5ObKX+QTs/MFs9PTWG7kGW5z+pidPmanj9np4Xki+Lx+pyU1NRWffPLJbysIM92bNabVHLH4Iy7DWVRjO9Zi7dq1RrdkGcxOH7PTU5cbAFd25Bluc/qYnT5mp4/Z6eF5Iri8HnGEhYUhISEhEL00eTbYYLdFIlSEAgDatGljcEfWwez0MTs9dbkBcGVHnuE2p4/Z6WN2+pidHp4ngsvrQcv+/fuRmJiIyMhIDBgwAAsXLkRycuOfta+srERlZaXr3yUlJfqdNgEVKMOn4v8Q8t/rHxQUFCA1NbXRZZmdO2anj9npqcstFKGIQSvl8szuN95scwCzq4/7qz5mp4/Z6eF5Iri8+vZx//79sXTpUqxZswYvvvgi8vPzMXjwYJSWlja6/MKFC9GiRQvXLSkpyS9NW1ELtEYq0nExBiEFvQAAI0eOZHYeYHb6mJ2e+rl1x8U4g3IAOG9uALOr4+02BzC7Otxf9TE7fcxOD88TwefVoGXkyJG48cYbkZaWhhEjRuCjjz7Cr7/+ilWrVjW6/KxZs3Dq1CnXraCgwC9NW1FbW3vE2zoixtYSrdEOAHDq1Clm5wFmp4/Z6amfWxtbAnpiAADggw/Of/kTZlfL220OYHZ1uL/qY3b6mJ0enieCz6dv0bds2RIpKSk4cOBAo3W73Q673e7LUzRpXbt2ZXaamJ0+Zue9cIQDAH788cfzLsPszk+2zQHMTob7qz5mp4/ZeY/nicDzadBSVlaGvLw8TJgwwV/9ICFLPm/BiKzeynWk4EtpfcH2kdL6G45PpfUr4/+fsgdP5svIz89H+/btlct5SnWdbU/ma0i/XX6d8+LB1dL6pDbn/2sqALy6Vz0fh/pK9/7PLu7zkz6v472yWGm9p11+/X3V7yf9ysuVPbTODv521zp7u7TeY5b82vgAkP3sOGldNS/EyVvK5E9wKEbZg8xZnAWAoF+EpGa3em6l9UUXSuu+ztPij3kf/L3NeaL943nKZeYk/kdaH/n+A9L65Fz5drtg/DJlDy/Pl8/74O/sQuPbSetrV2Qr16GaC00ltzpHWo/J/VW5DiPOEyopd8hfe3gifrv8PJL/ZA9pvRl2+NwDEPzsnN+1UC6T27fcp+c4VhLt0+NVjDpPeOLotIHSeuxB3+Y6G9E88POxAV4OWv72t7/h6quvhsPhQGFhITIzMxEaGoqxY+WTzhCQK75FHBIRiWaoQO0LLGbnGWanj9npqZ9bJU4jD7sBAKNHjza4M/PjNqeP2eljdvqYnR6eJ4LPq0HL4cOHMXbsWJw4cQJxcXEYNGgQPvvsM8TFxQWqvyajEqexCztQjSqEIwIA8MknnzA7DzA7fcxOT/3cImBHLFoDANq2bWtwZ+bHbU4fs9PH7PQxOz08TwSfV4OWt99+O1B9NHk9bZe4/v+sqMYmrEaXLvKPBVAtZqeP2empnxtQl90Rg7qxFm5z+pidPmanj9np4Xki+Ly6ehgREREREVGwcdBCRERERESmxkELERERERGZGgctRERERERkahy0EBERERGRqfk0uaRVbdmTIq2/1+ab4DQSZOmz5RNHAsDyef8jrasmU8t6SH5d92ZF/pn4yt9Uk/j5I7tdlfKJuq557RZpPTlbPvGqWc1ZLv+5AODSK76X1lO+OyOt55YXSOv5y+QTsllZ1DT5hF0LVnSX1lWTT6omIwQ8m0w32IqvVU9kNnKmfPLI/1z/tLSeEt5cWr989zXKHiJwULlMMHnS8/8Mkk8irJpId8qtU6X1kN2+T2hqVqGpislgE1+T1qdtk2/Xnky6aUbJ89Xntymb5NvNzCXLpfUbokukdU8mTVVNXm6ELWnqY12v7fJzrGpS9dxq+cSeNz77oLKHBPj+GobvtBARERERkalx0EJERERERKbGQQsREREREZkaBy1ERERERGRqHLQQEREREZGpcdBCRERERESmFtRLHgshAABnUQ2IYD6zO+dp+eVTK0rlFw0866xSPkeNqD7/41Fbq8vDE/7IrqZK/nMDQFmpU1o/W14pr1fLn+OsJBdPWDm7ikr5dlVzpolmp/i5AKC6XL5PnVH87NUV8sertktAnq9R2Xn0PDXyffJMmTy7Ertin/fheKeTW/3lfclOeNC3U7FtqvbpknDfjpcAEOLH7DzJTZXL2XL13zJV58iyKkUuZ+W5ny8TT1l5f1WegxW/P9lrD0+YOTunYrtRbZclQp6t6rUhYN3zhOocW6LY7sqq5fWaSv3svMpNBFFBQYFA7a+FN0AUFBQwO2bH7Cx0Y3aBz43Z6WfH3JgdszP+xuwCl5tNCC//BOYDp9OJwsJCxMTEwGazAQBKSkqQlJSEgoICxMbGBqsVr/i7RyEESktLkZiYiJAQzz6hd252v8fcAGbnC2anj9np0ckNYHbA72ebA5idLh7r9DE7fUbur0H9eFhISAg6duzYaC02Nta0v6A6/uyxRYsWXi1/vux+b7kBzM4XzE4fs9PjbW4As6vze9rmAGani8c6fcxOnxH7K7+IT0REREREpsZBCxERERERmZrhgxa73Y7MzEzY7XajWzkvM/Zoxp7OZdYezdpXfWbt0ax91WfWHs3aV31m7dGsfdVnxh7N2FNjzNinGXs6l1l7NGtf9Zm1R7P2VZ+RPQb1i/hERERERETeMvydFiIiIiIiIhkOWoiIiIiIyNQ4aCEiIiIiIlPjoIWIiIiIiEyNgxYiIiIiIjI1Qwctzz//PDp16oTIyEj0798fn3/+uZHtuJk3bx5sNpvbrXv37ka35cLs9DE7fcxOj5lzA5idLjPnBjA7XzA7fcxOj5lzA8yRnWGDlpUrV2L69OnIzMzEV199hV69emHEiBE4duyYUS01kJqaip9//tl127p1q9EtAWB2vmB2+pidHivkBjA7XWbMDWB2vmB2+pidHivkBpggO2GQfv36iSlTprj+XVNTIxITE8XChQuNaslNZmam6NWrl9FtNIrZ6WN2+pidHrPnJgSz02XW3IRgdr5gdvqYnR6z5yaEObIz5J2Wqqoq7Ny5ExkZGa77QkJCkJGRge3btxvRUqP279+PxMREdOnSBePHj8ehQ4eMbonZ+YDZ6WN2eqySG8DsdJktN4DZ+YLZ6WN2eqySG2B8doYMWoqLi1FTU4P4+Hi3++Pj43H06FEjWmqgf//+WLp0KdasWYMXX3wR+fn5GDx4MEpLSw3ti9npY3b6mJ0eK+QGMDtdZswNYHa+YHb6mJ0eK+QGmCO7sKA9k8WMHDnS9f9paWno378/HA4HVq1ahb/85S8GdmZ+zE4fs9PH7PQxOz3MTR+z08fs9DE7fWbIzpB3Wtq2bYvQ0FAUFRW53V9UVISEhAQjWlJq2bIlUlJScODAAUP7YHb6mJ0+ZqfHirkBzE6XGXIDmJ0vmJ0+ZqfHirkBxmRnyKAlIiICffr0wfr16133OZ1OrF+/HgMGDDCiJaWysjLk5eWhffv2hvbB7PQxO33MTo8VcwOYnS4z5AYwO18wO33MTo8VcwMMys6oKwC8/fbbwm63i6VLl4offvhBTJo0SbRs2VIcPXrUqJbcPPDAA2LTpk0iPz9f5OTkiIyMDNG2bVtx7Ngxo1tjdj5gdvqYnR6z5yYEs9Nl1tyEYHa+YHb6mJ0es+cmhDmyM2zQIoQQzz33nEhOThYRERGiX79+4rPPPjOyHTdjxowR7du3FxEREaJDhw5izJgx4sCBA0a35cLs9DE7fcxOj5lzE4LZ6TJzbkIwO18wO33MTo+ZcxPCHNnZhBAieO/rEBEREREReceQ77QQERERERF5ioMWIiIiIiIyNQ5aiIiIiIjI1DhoISIiIiIiU+OghYiIiIiITI2DFiIiIiIiMjUOWoiIiIiIyNQ4aCEiIiIiIlPjoIWIiIiIiEyNgxYiIiIiIjI1DlqIiIiIiMjU/j+gAMgeaqpwOgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 10\n",
    "\n",
    "sample_idx = np.random.choice(1797, N, replace=False)\n",
    "digits_sample = digits[sample_idx]\n",
    "targets_sample = targets[sample_idx]\n",
    "f, ax  = plt.subplots(1,10, figsize=(10, 5))\n",
    "\n",
    "for i in range(N):\n",
    "    ax[i].imshow(digits_sample[i].reshape(8,8))\n",
    "    ax[i].set_title('label: '+str(targets_sample[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ffpXAKqQ_vfg"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # Trainable parameters of the layer and their gradients\n",
    "        self.thetas = np.random.randn(input_size, output_size) # the weight matrix of the layer (W)\n",
    "        self.thetas_grads = np.empty_like(self.thetas) # gradient w.r.t. the weight matrix of the layer\n",
    "        self.bias = np.random.randn(output_size) # bias terms of the layer (b)\n",
    "        self.bias_grads = np.empty_like(self.bias) # gradient w.r.t. bias terms of the linear layer\n",
    "\n",
    "    def forward(self, x): \n",
    "        # keep x for backward computation\n",
    "        self.x = x\n",
    "        output = np.matmul(x, self.thetas) + self.bias\n",
    "        return output\n",
    "    \n",
    "    def backward(self, output_grad, learning_rate):\n",
    "        \"\"\"\n",
    "        Calculate and return gradient of the loss w.r.t. the input of linear layer given the input x and the gradient \n",
    "        w.r.t output of linear layer. You should also calculate and update gradients of layer parameters.\n",
    "        :param x: np.array, input tensor for linear layer;\n",
    "        :param output_grad: np.array, grad tensor w.r.t output of linear layer;\n",
    "        :return: np.array, grad w.r.t input of linear layer\n",
    "        \"\"\"\n",
    "        \n",
    "        ### BEGIN Solution (do not delete this comment)\n",
    "        input_grad = np.dot(output_grad, self.thetas.T)\n",
    "        self.thetas_grads = np.outer(self.x, output_grad)\n",
    "        self.bias_grads = np.squeeze(output_grad)\n",
    "        # >>> your solution here <<<\n",
    "        self.step(learning_rate)\n",
    "\n",
    "        ### END Solution (do not delete this comment!)\n",
    "        \n",
    "        return input_grad\n",
    "\n",
    "    def step(self, learning_rate):\n",
    "        self.thetas -= self.thetas_grads * learning_rate\n",
    "        self.bias -= self.bias_grads * learning_rate\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.thetas, self.bias\n",
    "    \n",
    "    def set_weights(self, thetas, bias):\n",
    "        self.thetas = thetas\n",
    "        self.bias = bias\n",
    "\n",
    "\n",
    "class LogisticActivation:\n",
    "    def __init__(self):\n",
    "        # the layer has no parameters\n",
    "        pass \n",
    "        \n",
    "    def sig(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # keep o for backward computation\n",
    "        self.o = self.sig(x)\n",
    "        return self.o \n",
    "\n",
    "\n",
    "    def backward(self,  output_grad, learning_rate=None):\n",
    "        \"\"\"\n",
    "        Calculate and return the gradient of the loss w.r.t. the input\n",
    "        of logistic non-linearity (given input x and the gradient \n",
    "        w.r.t output of logistic non-linearity).\n",
    "        \n",
    "        :param x: np.array, input tensor for logistic non-linearity;\n",
    "        :param output_grad: np.array, grad tensor w.r.t output of logistic non-linearity;\n",
    "        :return: np.array, grad w.r.t input of logistic non-linearity\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        input_grad = output_grad * self.o *(1-self.o )\n",
    "\n",
    "        \n",
    "        return input_grad\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_layer_size, output_size):\n",
    "        self.linear1 = Linear(input_size, hidden_layer_size)\n",
    "        self.activation1 = LogisticActivation()\n",
    "        self.linear2 = Linear(hidden_layer_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.linear1.forward(x)\n",
    "        h1a = self.activation1.forward(h1)\n",
    "        out = self.linear2.forward(h1a)\n",
    "        return out\n",
    "\n",
    "    def backward(self, output_grad, learning_rate):\n",
    "        \"\"\"\n",
    "        Calculate and return the gradient of the loss w.r.t. the input of MLP given the input and the gradient \n",
    "        w.r.t output of MLP. You should also update gradients of paramerters of MLP layers.\n",
    "        Hint - you should chain backward operations of modules you have already implemented. You may also\n",
    "        need to calculate intermediate forward results.\n",
    "        \n",
    "        :param x: np.array, input tensor for MLP;\n",
    "        :param output_grad: np.array, grad tensor w.r.t output of MLP;\n",
    "        :return: np.array, grad w.r.t input of MLP\n",
    "        \"\"\"\n",
    " \n",
    "        grad_l2 = self.linear2.backward(output_grad, learning_rate)\n",
    "        grad_sig = self.activation1.backward(grad_l2)\n",
    "        out = self.linear1.backward(grad_sig, learning_rate)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def get_weights(self):\n",
    "\n",
    "        return {\"1 layer\" : self.linear1.get_weights(),\n",
    "                \"2 layer\" : self.linear2.get_weights()}\n",
    "    \n",
    "    \n",
    "    def set_weights(self, w_h1, b_h1, w_h2, b_h2):\n",
    "        self.linear1.set_weights(w_h1, b_h1)\n",
    "        self.linear2.set_weights(w_h2, b_h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "07DUqp86_0To"
   },
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits, reference_answers):\n",
    "\n",
    "    true_answer = reference_answers\n",
    "    reference_answers = np.zeros(10)\n",
    "    reference_answers[true_answer] = 1\n",
    "\n",
    "    exps = np.exp(logits-np.max(logits))\n",
    "    softmax_probs = exps / np.sum(exps)\n",
    "    \n",
    "    loss = -np.sum(reference_answers*np.log(softmax_probs+1e-9))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def grad_softmax_crossentropy_with_logits(logits, reference_answers):\n",
    "\n",
    "    true_answer = reference_answers\n",
    "    reference_answers = np.zeros(10)\n",
    "    reference_answers[true_answer] = 1\n",
    "\n",
    "    exps = np.exp(logits-np.max(logits))\n",
    "    softmax_probs = exps / np.sum(exps)\n",
    "\n",
    "    grad = (softmax_probs - reference_answers)\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DWkD2V1y_4QU",
    "outputId": "025c7788-1647-41b7-b6c2-3d57dbf1663c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0 , Loss : 2.71 , Accuracy on train: 0.103 , Accuracy on test: 0.0667\n",
      "Starting epoch 10 , Loss : 2.15 , Accuracy on train: 0.625 , Accuracy on test: 0.649\n",
      "Starting epoch 20 , Loss : 1.55 , Accuracy on train: 0.821 , Accuracy on test: 0.807\n",
      "Starting epoch 30 , Loss : 1.07 , Accuracy on train: 0.875 , Accuracy on test: 0.871\n",
      "Starting epoch 40 , Loss : 0.742 , Accuracy on train: 0.9 , Accuracy on test: 0.889\n",
      "Starting epoch 50 , Loss : 0.534 , Accuracy on train: 0.911 , Accuracy on test: 0.904\n",
      "Starting epoch 60 , Loss : 0.399 , Accuracy on train: 0.92 , Accuracy on test: 0.924\n",
      "Starting epoch 70 , Loss : 0.308 , Accuracy on train: 0.927 , Accuracy on test: 0.931\n",
      "Starting epoch 80 , Loss : 0.245 , Accuracy on train: 0.929 , Accuracy on test: 0.931\n",
      "Starting epoch 90 , Loss : 0.198 , Accuracy on train: 0.936 , Accuracy on test: 0.933\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "mlp = MLP(input_size=input_size, hidden_layer_size=100, output_size=classes_n)\n",
    "\n",
    "epochs_n = 100\n",
    "learning_curve = [0] * epochs_n\n",
    "test_curve = [0] * epochs_n\n",
    "\n",
    "x_train = digits_train\n",
    "x_test = digits_test\n",
    "y_train = targets_train\n",
    "y_test = targets_test\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "for epoch in range(epochs_n):\n",
    "    \n",
    "    y_pred = []\n",
    "\n",
    "    for sample_i in range(train_size):\n",
    "        x = x_train[sample_i].reshape((1, -1))\n",
    "        target = np.array([y_train[sample_i]])\n",
    "\n",
    "        logits = mlp.forward(x)\n",
    "        \n",
    "        loss = softmax_crossentropy_with_logits(logits, target)\n",
    "\n",
    "        grads_input_sml = grad_softmax_crossentropy_with_logits(logits, target)\n",
    "\n",
    "        mlp.backward(grads_input_sml, learning_rate)\n",
    "\n",
    "        y_pred.extend(logits.argmax(1))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        \n",
    "        y_pred_test = []\n",
    "\n",
    "        for sample_i in range(test_size):\n",
    "            x = x_test[sample_i].reshape((1, -1))\n",
    "            target = np.array([y_test[sample_i]])\n",
    "            \n",
    "            logits = mlp.forward(x)\n",
    "            y_pred_test.extend(logits.argmax(1))\n",
    "\n",
    "        print('Starting epoch {}'.format(epoch), \\\n",
    "              ', Loss : {:.3}'.format(loss), \\\n",
    "              ', Accuracy on train: {:.3}'.format(accuracy_score(y_train, y_pred)), \\\n",
    "              ', Accuracy on test: {:.3}'.format(accuracy_score(y_test, y_pred_test)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_accuracy(nn, y_test):\n",
    "    \n",
    "    y_test = np.array(y_test)\n",
    "    y_pred_test = np.array([])\n",
    "\n",
    "    for sample_i in range(y_test.size):\n",
    "        x = x_test[sample_i].reshape((1, -1))\n",
    "\n",
    "        logits = nn.forward(x)\n",
    "        y_pred_test = np.append(y_pred_test, logits.argmax(1))\n",
    "\n",
    "    return accuracy_score(y_test, y_pred_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0DNQhxXaARCy",
    "outputId": "696177f9-461a-40d4-bfd8-a984c4b528cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 39  0  0  0  0  2  0  0  2]\n",
      " [ 0  1 41  1  0  0  0  1  0  0]\n",
      " [ 0  0  0 43  0  0  0  0  1  1]\n",
      " [ 0  0  0  0 37  0  0  1  0  0]\n",
      " [ 0  0  0  0  0 46  0  0  0  2]\n",
      " [ 0  1  0  0  0  0 51  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 48  0  0]\n",
      " [ 0  4  1  0  0  1  0  1 41  0]\n",
      " [ 0  0  0  4  0  2  0  0  1 40]]\n"
     ]
    }
   ],
   "source": [
    "logits = mlp.forward(x_test)\n",
    "exps = np.exp(logits-np.max(logits))\n",
    "softmax_probs = exps / np.sum(exps)\n",
    "\n",
    "pred = np.zeros((y_test.shape[0]), dtype=\"int\")\n",
    "\n",
    "for idx, prob in enumerate(softmax_probs):\n",
    "    pred[idx] = np.argmax(prob)\n",
    "\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n",
      "Accuracy on test, initial mlp: 0.94\n"
     ]
    }
   ],
   "source": [
    "w_1 = mlp.get_weights()['1 layer'][0]\n",
    "b_1 = mlp.get_weights()['1 layer'][1]\n",
    "w_2 = mlp.get_weights()['2 layer'][0]\n",
    "b_2 = mlp.get_weights()['2 layer'][1]\n",
    "\n",
    "for m in [w_1, b_1, w_2, b_2]:\n",
    "    print(m.shape)\n",
    "print('Accuracy on test, initial mlp: {:.4}'.format(MLP_accuracy(mlp, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test, modified b2: 0.7778\n"
     ]
    }
   ],
   "source": [
    "mlp_modified = copy.deepcopy(mlp)\n",
    "mlp_modified.set_weights(w_1, b_1, w_2, np.random.normal(size=10))\n",
    "print('Accuracy on test, modified b2: {:.4}'.format(MLP_accuracy(mlp_modified, y_test)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PART_2_of_HW2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
